{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You need to know some basic calculus in order to understand how functions change over time (derivatives), and to calculate the total amount of a quantity that accumulates over a time period (integrals). The language of calculus will allow you to speak precisely about the properties of functions and better understand their behaviour.\n",
    "\n",
    "Normally taking a calculus course involves doing lots of tedious calculations by hand, but having the power of computers on your side can make the process much more fun. This section describes the key ideas of calculus which you'll need to know to understand machine learning concepts.\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "A derivative can be defined in two ways:\n",
    "\n",
    "1. Instantaneous rate of change (Physics)\n",
    "2. Slope of a line at a specific point (Geometry)\n",
    "\n",
    "Both represent the same principle, but for our purposes it's easier to explain using the geometric definition.\n",
    "\n",
    "### Geometric definition\n",
    "\n",
    "In geometry slope represents the steepness of a line. It answers the question: how much does or change given a specific change in ?\n",
    "\n",
    "![slope_formula.png](./assets/slopeFormula.png)\n",
    "\n",
    "Using this definition we can easily calculate the slope between two points. But what if I asked you, instead of the slope between two points, what is the slope at a single point on the line? In this case there isn't any obvious \"rise-over-run\" to calculate. Derivatives help us answer this question.\n",
    "\n",
    "A derivative outputs an expression we can use to calculate the _instantaneous rate of change_, or slope, at a single point on a line. After solving for the derivative you can use it to calculate the slope at every other point on the line.\n",
    "\n",
    "### Taking the derivative\n",
    "\n",
    "Consider the graph below, where .\n",
    "\n",
    "![calculus_slope_intro.png](./assets/takingDerivative.png)\n",
    "\n",
    "The slope between (1,4) and (3,12) would be:\n",
    "\n",
    "![calculus_slope_intro.png](./assets/takingDerivative1.png)\n",
    "\n",
    "But how do we calculate the slope at point (1,4) to reveal the change in slope at that specific point?\n",
    "\n",
    "One way would be to find the two nearest points, calculate their slopes relative to and take the average. But calculus provides an easier, more precise way: compute the derivative. Computing the derivative of a function is essentially the same as our original proposal, but instead of finding the two closest points, we make up an imaginary point an infinitesimally small distance away from and compute the slope between and the new point.\n",
    "\n",
    "In this way, derivatives help us answer the question: how does change if we make a very very tiny increase to x? In other words, derivatives help _estimate_ the slope between two points that are an infinitesimally small distance away from each other. A very, very, very small distance, but large enough to calculate the slope.\n",
    "\n",
    "In math language we represent this infinitesimally small increase using a limit. A limit is defined as the output value a function approaches as the input value approaches another value. In our case the target value is the specific point at which we want to calculate slope.\n",
    "\n",
    "### Step-by-step\n",
    "\n",
    "Calculating the derivative is the same as calculating normal slope, however in this case we calculate the slope between our point and a point infinitesimally close to it. We use the variable to represent this infinitesimally distance. Here are the steps:\n",
    "\n",
    "1. Given the function:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = x^2\n",
    "\\end{equation*}\n",
    "\n",
    "2. Increment $x$ by a very small value $h(h=\\Delta x)$ \n",
    "\n",
    "\\begin{equation*}\n",
    "f(x+h) = (x+h)^2\n",
    "\\end{equation*}\n",
    "\n",
    "3. Apply the slope formula \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {f(x+h) - f(x)}{h}\n",
    "\\end{equation*}\n",
    "\n",
    "4. Simplify the equation \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {x^2 + 2xh + h^2 -x^2}{h}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {2xh + h^2}{h} = 2x + h\n",
    "\\end{equation*}\n",
    "\n",
    "5. set h to 0 (the limit as h heads towards 0)\n",
    "\n",
    "\\begin{equation*}\n",
    "2x + 0 = 2x\n",
    "\\end{equation*}\n",
    "\n",
    "so what does this mean? it means for the function $f(x)=x^2$, the slope at any point equals $2x$. The formula defined as:  \n",
    "\n",
    "$$\\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "\n",
    "Code\n",
    "\n",
    "Let's write code to calculate the derivative of any function $f(x)$. We test our function works as expected on the input $f(x)=x^2$ producing a value close to the actual derivative $2x$.\n",
    "\n",
    "```\n",
    "def get_derivative(func, x):\n",
    "    \"\"\"Compute the derivative of `func` at the location `x`.\"\"\"\n",
    "    h = 0.0001                          # step size\n",
    "    return (func(x+h) - func(x)) / h    # rise-over-run\n",
    "\n",
    "def f(x): return x**2                   # some test function f(x)=x^2\n",
    "x = 3                                   # the location of interest\n",
    "computed = get_derivative(f, x)\n",
    "actual = 2*x\n",
    "\n",
    "computed, actual   # = 6.0001, 6        # pretty close if you ask me...\n",
    "```\n",
    "\n",
    "In general it's preferable to use the math to obtain exact derivative formulas, but keep in mind you can always compute derivatives numerically by computing the rise-over-run for a \"small step\" .\n",
    "\n",
    "### Machine learning use cases\n",
    "\n",
    "Machine learning uses derivatives in optimization problems. Optimization algorithms like _gradient descent_ use derivatives to decide whether to increase or decrease weights in order to maximize or minimize some objective (e.g. a model's accuracy or error functions). Derivatives also help us approximate nonlinear functions as linear functions (tangent lines), which have constant slopes. With a constant slope we can decide whether to move up or down the slope (increase or decrease our weights) to get closer to the target value (class label).\n",
    "\n",
    "## Chain rule\n",
    "\n",
    "The chain rule is a formula for calculating the derivatives of composite functions. Composite functions are functions composed of functions inside other function(s).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Given a composite function $f(x)=A(B(x))$, the derivative of $f(x)$ equals the product of the derivative of $A$ with respect to $B(x)$ and the derivative of $B$ with respect to $x$.\n",
    "\n",
    "\\begin{equation*}\n",
    "compositeFunctionDerivative=outerFunctionDerivativeâˆ—innerFunctionDerivative\n",
    "\\end{equation*}\n",
    "\n",
    "For example, given a composite function $f(x)$, where:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = h(g(x))\n",
    "\\end{equation*}\n",
    "\n",
    "The chain rule tells us that the derivative of $f(x)$ equals:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {df}{dx} = \\frac {dh}{dg} * \\frac {dg}{dx}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "![calculus_slope_intro.png](./assets/stepByStep.PNG)\n",
    "\n",
    "![calculus_slope_intro.png](./assets/multipleFunctions.PNG)\n",
    "\n",
    "\n",
    "## Gradients\n",
    "\n",
    "A gradient is a vector that stores the partial derivatives of multivariable functions. It helps us calculate the slope at a specific point on a curve for functions with multiple independent variables. In order to calculate this more complex slope, we need to isolate each variable to determine how it impacts the output on its own. To do this we iterate through each of the variables and calculate the derivative of the function after holding all other variables constant. Each iteration produces a partial derivative which we store in the gradient.\n",
    "\n",
    "### Partial derivatives\n",
    "\n",
    "In functions with 2 or more variables, the partial derivative is the derivative of one variable with respect to the others. If we change , but hold all other variables constant, how does change? That's one partial derivative. The next variable is . If we change but hold constant, how does change? We store partial derivatives in a gradient, which represents the full derivative of the multivariable function.\n",
    "\n",
    "\n",
    "![calculus_slope_intro.png](./assets/partialDerivatives.PNG)\n",
    "\n",
    "\n",
    "### Useful properties\n",
    "\n",
    "There are two additional properties of gradients that are especially useful in deep learning. The gradient of a function:\n",
    "\n",
    "\n",
    "1. Always points in the direction of greatest increase of a function ([explained here](https://betterexplained.com/articles/understanding-pythagorean-distance-and-the-gradient))\n",
    "2. Is zero at a local maximum or local minimum\n",
    "\n",
    "References\n",
    "\n",
    "| [1] | [https://en.wikipedia.org/wiki/Derivative](https://en.wikipedia.org/wiki/Derivative) |\n",
    "\n",
    "| [[2]](https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#id7) | [https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction) |\n",
    "\n",
    "| [3] | [https://en.wikipedia.org/wiki/Partial_derivative](https://en.wikipedia.org/wiki/Partial_derivative) |\n",
    "\n",
    "| [4] | [https://en.wikipedia.org/wiki/Gradient](https://en.wikipedia.org/wiki/Gradient) |\n",
    "\n",
    "| [5] | [https://betterexplained.com/articles/vector-calculus-understanding-the-gradient](https://betterexplained.com/articles/vector-calculus-understanding-the-gradient) |\n",
    "\n",
    "| [6] | [https://www.mathsisfun.com/calculus/derivatives-introduction.html](https://www.mathsisfun.com/calculus/derivatives-introduction.html) |\n",
    "\n",
    "| [7] | [http://tutorial.math.lamar.edu/Classes/CalcI/DefnOfDerivative.aspx](http://tutorial.math.lamar.edu/Classes/CalcI/DefnOfDerivative.aspx) |\n",
    "\n",
    "| [8] | [https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction](https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction) |\n",
    "\n",
    "| [9] | [http://tutorial.math.lamar.edu/Classes/CalcI/ChainRule.aspx](http://tutorial.math.lamar.edu/Classes/CalcI/ChainRule.aspx) |\n",
    "\n",
    "| [10] | [https://youtu.be/pHMzNW8Agq4?t=1m5s](https://youtu.be/pHMzNW8Agq4?t=1m5s) |\n",
    "\n",
    "| [[11]](https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#id6) | [https://en.wikipedia.org/wiki/Dot_product](https://en.wikipedia.org/wiki/Dot_product) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
